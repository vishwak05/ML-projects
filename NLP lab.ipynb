{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26219442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the first sentence.this is the second sentence.is this third\n",
      "this is the first sentence. this is the second sentence. is this third?\n",
      "['this', 'is', 'the', 'first', 'sentence', '.', 'this', 'is', 'the', 'second', 'sentence', '.', 'is', 'this', 'third', '?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tkrcet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#week1\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text1=\"this is the first sentence.this is the second sentence.is this third\"\n",
    "print(text1)\n",
    "sent_tokenize(text1)\n",
    "len(sent_tokenize(text1))\n",
    "text1=\"this is the first sentence. this is the second sentence. is this third?\"\n",
    "print(text1)\n",
    "sent_tokenize(text1)\n",
    "len(sent_tokenize(text1))\n",
    "word_tokenize(text1)\n",
    "len(word_tokenize(text1))\n",
    "word_result=word_tokenize(text1)\n",
    "print(word_result)\n",
    "word_result[1:5]\n",
    "word_result[:5]\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "pst=PunktSentenceTokenizer()\n",
    "pst.tokenize(text1)\n",
    "len(pst.tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c18b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Forms for : 'talk':talks, talked, talking\n",
      "Word Forms for : 'run':runs, runer, runing\n",
      "Word Forms for : 'jump':jumps, jumped, jumping\n",
      "Word Forms for : 'play':plays, player, playing\n"
     ]
    }
   ],
   "source": [
    "#week2\n",
    "\n",
    "word_suffixes={\n",
    "    \"talk\": [\"s\",\"ed\",\"ing\"],\n",
    "    \"run\" : [\"s\",\"er\",\"ing\"],\n",
    "    \"jump\" : [\"s\",\"ed\",\"ing\"],\n",
    "    \"play\" : [\"s\",\"er\",\"ing\"],\n",
    "}\n",
    "def generate_word_froms(root,suffixes):\n",
    "    word_forms=[root + suffix for suffix in suffixes]\n",
    "    return word_forms\n",
    "for root, suffixes in word_suffixes.items():\n",
    "    generated_forms= generate_word_froms(root,suffixes)\n",
    "    print(f\"Word Forms for : '{root}':{', '.join(generated_forms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63844bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumps -> jump\n",
      "jumping -> jump\n",
      "played -> play\n",
      "playing -> play\n"
     ]
    }
   ],
   "source": [
    "#week3\n",
    "\n",
    "add_delete_table={\n",
    "    \"s\": \"\",\n",
    "    \"es\": \"\",\n",
    "    \"ed\": \"\",\n",
    "    \"ing\" : \"\",\n",
    "}\n",
    "def stem_word(word):\n",
    "    for suffix,replacement in add_delete_table.items():\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)] + replacement\n",
    "    return word\n",
    "\n",
    "words=[\"jumps\",\"jumping\",\"played\",\"playing\"]\n",
    "\n",
    "stemmed_words = [stem_word(word) for word in words]\n",
    "\n",
    "for word, stemmed in zip(words,stemmed_words):\n",
    "    print(f\"{word} -> {stemmed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cb18b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: ['Indian', 'Institute', 'Technology', 'IIT']\n",
      "\n",
      "After Extracting the parts\n",
      "\n",
      " (S\n",
      "  (NP This/DT tree/NN)\n",
      "  (VP (V is/VBZ))\n",
      "  (VP\n",
      "    (V illustrating/VBG)\n",
      "    (NP the/DT constituency/NN)\n",
      "    (NP relation/NN))) \n",
      "\n",
      "Subject: Mary\n",
      "Verb: likes\n",
      "Object: Mary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tkrcet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\tkrcet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\tkrcet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#week4\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "# Sample text\n",
    "text = \"I visited the Indian Institute of Technology (IIT) last summer.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Part-of-speech tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "named_entities = nltk.ne_chunk(tagged_tokens)\n",
    "\n",
    "# Extracting named entities\n",
    "entities = []\n",
    "for entity in named_entities:\n",
    "    if isinstance(entity, nltk.Tree):\n",
    "        entities.append(\" \".join([word for word, tag in entity.leaves()]))\n",
    "print(\"Named Entities:\", entities)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "\n",
    "# String to parse\n",
    "to_parse = \"This tree is illustrating the constituency relation\"\n",
    "\n",
    "# Find all parts of speech in above sentence\n",
    "tagged_parts = pos_tag(word_tokenize(to_parse))\n",
    "\n",
    "# Defining grammar on basis of which we 've to extract\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}\n",
    "    P: {<IN>}\n",
    "    V: {<V.*>}\n",
    "    PP: {<p> <NP>}\n",
    "    VP: {<V> <NP|PP>*}\"\"\"\n",
    "\n",
    "# Extracting all parts of speech\n",
    "parser = RegexpParser(grammar)\n",
    "\n",
    "# Print all parts of speech in above sentence\n",
    "output = parser.parse(tagged_parts)\n",
    "print(\"\\nAfter Extracting the parts\\n\\n\", output,\"\\n\")\n",
    "\n",
    "\n",
    "#semantic parsing\n",
    "def extract_semantic_info(sentence):\n",
    "    words = sentence.split()\n",
    "    subject = \"\"\n",
    "    verb = \"\"\n",
    "    object_ = \"\"\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i].lower() in [\"he\", \"she\", \"it\", \"they\", \"john\", \"mary\"]:\n",
    "            subject = words[i]\n",
    "        elif words[i].lower() in [\"likes\", \"loves\", \"hates\", \"adores\"]:\n",
    "            verb = words[i]\n",
    "        if i+1 < len(words):\n",
    "            object_=words[i+1]\n",
    "\n",
    "    return subject, verb, object_\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"John likes Mary\"\n",
    "\n",
    "# Extract semantic information\n",
    "subject, verb, object_ = extract_semantic_info(sentence)\n",
    "\n",
    "# Print semantic information\n",
    "print(\"Subject:\", subject)\n",
    "print(\"Verb:\", verb)\n",
    "print(\"Object:\", object_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c483331e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tkrcet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['The', 'dogs', 'are', 'barking', 'loudly', 'outside', '.']\n",
      "Lemmatized tokens: ['The', 'dog', 'are', 'barking', 'loudly', 'outside', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Tutorials', 'Point']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#week5\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "# Sample text\n",
    "text = \"The dogs are barking loudly outside.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Initialize WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize tokens\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Lemmatized tokens:\", lemmatized_tokens)\n",
    "\n",
    "\n",
    "import os, os.path\n",
    "path = os.path.expanduser('~/natural_language_toolkit_data')\n",
    "if not os.path.exists(path):\n",
    "   os.mkdir(path)\n",
    "os.path.exists(path)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "exptagger = DefaultTagger('NN')\n",
    "exptagger.tag(['Tutorials','Point'])\n",
    "\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "exptagger = DefaultTagger('NN')\n",
    "exptagger.tag_sents([['Hi', ','], ['How', 'are', 'you', '?']])\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import untag\n",
    "untag([('Tutorials', 'NN'), ('Point', 'NN')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0196dd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize', 'this', 'sentence', 'using', 'NLTK', '.']\n",
      "[('This', 'DT'), ('is', 'VBZ'), ('simple', 'JJ'), ('POS', 'NNP'), ('tagging', 'VBG'), ('example', 'NN'), ('.', '.')]\n",
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP City/NNP)\n",
      "  ./.)\n",
      "Neutral Sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\tkrcet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#week6\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence= \"Tokenize this sentence using NLTK.\"\n",
    "tokens=word_tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence=\"This is simple POS tagging example.\"\n",
    "tokens=word_tokenize(sentence)\n",
    "tagged=nltk.pos_tag(tokens)\n",
    "print(tagged)\n",
    "\n",
    "\n",
    "#NAMED ENTITY RECOGNITION\n",
    "import nltk\n",
    "sentence=\"John works at Google in New York City.\"\n",
    "tokens=nltk.word_tokenize(sentence)\n",
    "tagged=nltk.pos_tag(tokens)\n",
    "entities=nltk.ne_chunk(tagged)\n",
    "print(entities)\n",
    "\n",
    "\n",
    "#SENTIMENTAL ANAKLYSIS\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sentence=\"This is a good movie.\"\n",
    "analyzer=SentimentIntensityAnalyzer()\n",
    "sentiment_scores=analyzer.polarity_scores(sentence)\n",
    "\n",
    "if sentiment_scores['compound']>=0.5:\n",
    "    print(\"Positive Sentiment\")\n",
    "elif sentiment_scores['compound']<=-0.5:\n",
    "    print(\"Negative Sentiment\")\n",
    "else:\n",
    "    print(\"Neutral Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2bc1ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Machine', 'learning'), ('learning', 'is'), ('is', 'an'), ('an', 'important'), ('important', 'part'), ('part', 'of'), ('of', 'AIand'), ('AIand', 'AI'), ('AI', 'is'), ('is', 'going'), ('going', 'to'), ('to', 'become'), ('become', 'inmportant'), ('inmportant', 'for'), ('for', 'daily'), ('daily', 'functionong')]\n",
      "['BBDO South', 'Georgia-Pacific']\n",
      "['This', 'is', 'a', 'dog', 'This', 'is', 'a', 'cat', 'I', 'love', 'my', 'cat', 'This', 'is', 'my', 'name']\n",
      "\n",
      " All the possible Bigrams are \n",
      "[('This', 'is'), ('is', 'a'), ('a', 'dog'), ('This', 'is'), ('is', 'a'), ('a', 'cat'), ('I', 'love'), ('love', 'my'), ('my', 'cat'), ('This', 'is'), ('is', 'my'), ('my', 'name')]\n",
      "\n",
      " Bigrams along with their frequency \n",
      "{('This', 'is'): 3, ('is', 'a'): 2, ('a', 'dog'): 1, ('a', 'cat'): 1, ('I', 'love'): 1, ('love', 'my'): 1, ('my', 'cat'): 1, ('is', 'my'): 1, ('my', 'name'): 1}\n",
      "\n",
      " Unigrams along with their frequency \n",
      "{'This': 3, 'is': 3, 'a': 2, 'dog': 1, 'cat': 2, 'I': 1, 'love': 1, 'my': 2}\n",
      "\n",
      " Bigrams along with their probability\n",
      "{('This', 'is'): 1.0, ('is', 'a'): 0.6666666666666666, ('a', 'dog'): 0.5, ('a', 'cat'): 0.5, ('I', 'love'): 1.0, ('love', 'my'): 1.0, ('my', 'cat'): 0.5, ('is', 'my'): 0.3333333333333333, ('my', 'name'): 0.5}\n",
      "\n",
      " The bigrams in the given sentence are\n",
      "[('This', 'is'), ('is', 'my'), ('my', 'cat')]\n",
      "\n",
      "Probability of the sentence \"This is my cat\" = 0.16666666666666666\n",
      "0.05555555555555555\n",
      "('<s>', 'John') 1 3\n",
      "('John', 'read') 1 1\n",
      "('read', 'a') 2 3\n",
      "('a', 'book') 1 2\n",
      "('book', '</s>') 1 2\n"
     ]
    }
   ],
   "source": [
    "#week7\n",
    "\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "s=\"Machine learning is an important part of AI\"\"and AI is going to become inmportant for daily functionong\" \n",
    "tokens=[token for token in s.split(\" \")]\n",
    "output=list(ngrams(tokens,2))\n",
    "print(output)\n",
    "\n",
    "locs = [('Omnicom', 'IN', 'New York'), ('DDB Needham', 'IN', 'New York'), ('Kaplan Thaler Group', 'IN', 'New York'), ('BBDO South', 'IN', 'Atlanta'), ('Georgia-Pacific', 'IN', 'Atlanta')]\n",
    "query = [e1 for (e1, rel, e2) in locs if e2 == 'Atlanta']\n",
    "print(query)\n",
    "\n",
    "\n",
    "def readData():\n",
    "    data = ['This is a dog', 'This is a cat', 'I love my cat', 'This is my name']\n",
    "    dat = []\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i].split():\n",
    "            dat.append(word)\n",
    "    print(dat)\n",
    "    return dat\n",
    "\n",
    "def createBigram(data):\n",
    "    listOfBigrams = []\n",
    "    bigramCounts = {}\n",
    "    unigramCounts = {}\n",
    "    for i in range(len(data)-1):\n",
    "      if i <len(data)-1 and data[i+1].islower():\n",
    "          listOfBigrams.append((data[i],data[i+1]))\n",
    "          if(data[i],data[i+1]) in bigramCounts:\n",
    "            bigramCounts[(data[i],data[i+1])] += 1\n",
    "          else:\n",
    "            bigramCounts[(data[i],data[i+1])] = 1\n",
    "      if data[i] in unigramCounts:\n",
    "            unigramCounts[data[i]] += 1\n",
    "      else:\n",
    "            unigramCounts[data[i]] = 1\n",
    "    return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1= bigram[0]\n",
    "        word2= bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram)) /(unigramCounts.get(word1))\n",
    "    return listOfProb\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data=readData()\n",
    "    listOfBigrams, unigramCounts, bigramCounts=createBigram(data)\n",
    "    \n",
    "    print(\"\\n All the possible Bigrams are \")\n",
    "    print(listOfBigrams)\n",
    "    print(\"\\n Bigrams along with their frequency \")\n",
    "    print(bigramCounts)\n",
    "    print(\"\\n Unigrams along with their frequency \")\n",
    "    print(unigramCounts)\n",
    "    bigramProb=calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "    print(\"\\n Bigrams along with their probability\")\n",
    "    print(bigramProb)\n",
    "    inputList=\"This is my cat\"\n",
    "    splt=inputList.split()\n",
    "    outputProb1= 1\n",
    "    bilist= []\n",
    "    bigrm =[]\n",
    "    for i in range (len(splt) - 1):\n",
    "        if i < len(splt) - 1:\n",
    "            bilist.append((splt[i], splt[i+1]))\n",
    "    print(\"\\n The bigrams in the given sentence are\")\n",
    "    print(bilist)\n",
    "\n",
    "    outputProb1 = 1  # Start with a probability of 1\n",
    "    for bigram in bilist:\n",
    "        if bigram in bigramProb:\n",
    "            outputProb1 *= bigramProb[bigram]  # Multiply by the probability of the bigram if it exists\n",
    "        else:\n",
    "            outputProb1 *= 0  # Multiply by 0 if the bigram does not exist in the model\n",
    "\n",
    "    print('\\n' + 'Probability of the sentence \"This is my cat\" = ' + str(outputProb1))\n",
    "\n",
    "    \n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce, partial\n",
    "from operator import mul\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "def prob_product(prob_list):\n",
    "  return reduce(mul, prob_list)\n",
    "\n",
    "text = [['<s>', 'John', 'read', 'Moby', 'Dick', '</s>'],\n",
    "        ['<s>', 'Mary', 'read', 'a', 'different', 'book', '</s>'],\n",
    "        ['<s>', 'She', 'read', 'a', 'book', 'by', 'Cher', '</s>']]\n",
    "\n",
    "bigram_counts=sum([Counter(ngrams(t, 2)) for t in text], Counter())\n",
    "unigram_counts=sum([Counter(ngrams(t, 1)) for t in text], Counter())\n",
    "\n",
    "count_S_John= bigram_counts[('<s>', 'John')]\n",
    "count_S=unigram_counts[('<s>',)]\n",
    "\n",
    "sentence = '<s> John read a book </s>'.split()\n",
    "\n",
    "prob_S_John_read_a_book=prob_product([bigram_counts[bg]/unigram_counts[bg[:-1]] \n",
    "                                      for bg in ngrams(sentence,2)])\n",
    "print(prob_S_John_read_a_book)\n",
    "\n",
    "for bg in ngrams(sentence,2):\n",
    "  print(bg,bigram_counts[bg], unigram_counts[bg[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33c3daa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tkrcet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tkrcet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('everything', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('same', 'JJ')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#week 8\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "text=nltk.word_tokenize(\"And now for everything completely same\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0203e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sukanya', 'NNP'), (',', ','), ('Raj', 'NNP'), ('Neha', 'NNP'), ('good', 'JJ'), ('friendsSukanya', 'NN'), ('getting', 'VBG'), ('married', 'VBD'), ('next', 'JJ'), ('yearMarraige', 'NN'), ('big', 'JJ'), ('step', 'NN'), ('one', 'CD'), (\"'s\", 'POS'), ('lifeIt', 'NN'), ('exciting', 'VBG'), ('frighteningBut', 'JJ'), ('friendship', 'NN'), ('sacred', 'VBD'), ('bonf', 'JJ'), ('peopleIt', 'NN'), ('special', 'JJ'), ('kind', 'NN'), ('clone', 'NN'), ('usMany', 'NN'), ('must', 'MD'), ('tried', 'VB'), ('seaching', 'VBG'), ('friendBut', 'JJ'), ('never', 'RB'), ('found', 'VBD'), ('right', 'RB'), ('one', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "#week 9\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "text=\"Sukanya, Raj and Neha are my good friends\"\\\n",
    "\"Sukanya is getting married next year\"\\\n",
    "\"Marraige is a big step in one's life\"\\\n",
    "\"It is both exciting and frightening\"\\\n",
    "\"But friendship is a sacred bonf between people\"\\\n",
    "\"It is a special kind of clone between us\"\\\n",
    "\"Many of you must have tried seaching for a friend\"\\\n",
    "\"But never found the right one\"\n",
    "\n",
    "tokenized=sent_tokenize(text)\n",
    "for i in tokenized:\n",
    "    wordsList=nltk.word_tokenize(i)\n",
    "    wordsList=[w for w in wordsList if not w in stop_words]\n",
    "    tagged=nltk.pos_tag(wordsList)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ee29af6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#week 10\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstructures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Documents\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtagging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MlTagger\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstemming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'core'"
     ]
    }
   ],
   "source": [
    "#week 10\n",
    "\n",
    "import sys\n",
    "from core.structures import Documents\n",
    "from preprocessing.tagging import MlTagger\n",
    "from preprocessing.stemming import PorterStemmer\n",
    "def process(raw_doc,pipeline=[\"sentence\",'pos','stemming']):\n",
    "    document=raw_doc\n",
    "    if 'sentence' in pipeline:\n",
    "        document=Document(document)\n",
    "    if 'pos' in pipeline:\n",
    "        tagger=MLTagger()\n",
    "        if isinstance(document,Document):\n",
    "            sent=[tagger.tag(sentence) for sentence in document]\n",
    "            document.sent=sent\n",
    "        else:\n",
    "            document=tagger.tag(document)\n",
    "    if 'stemming' in pipeline:\n",
    "        stemmer=PorterStemmer()\n",
    "        if isinstance(document,Document):\n",
    "            for sent in document.sent:\n",
    "                for tokenid in range(len(sent)):\n",
    "                    sent[tokenid].replace(stemmer.stem(sent[tokenid]))\n",
    "        else:\n",
    "            for token_id in range(len(document)):\n",
    "                document[token_id]=stemmer.stem(sent[token_id])\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76b13a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "#week 11\n",
    "\n",
    "\n",
    "import nltk\n",
    "sentence=[('the','DT'),('little','JJ'),('yellow','JJ'),('dog','NN'),('barked','VBD'),('at','IN'),('the','DT'),('cat','NN')]\n",
    "\n",
    "grammar=\"NP:{<DT>?<JJ>*<NN>}\"\n",
    "cp=nltk.RegexpParser(grammar)\n",
    "res=cp.parse(sentence)\n",
    "print(res)\n",
    "res.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64420281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module 'nltk' has no attribute 'regexpParser'\n"
     ]
    }
   ],
   "source": [
    "#week 12\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text=state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text=state_union.raw(\"2006-GWBush.txt\")\n",
    "custom_sent_tokenizer=PunktSentenceTokenizer(train_text)\n",
    "tokenized=custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words=nltk.word_tokenize(i)\n",
    "            tagged=nltk.pos_tag(words)\n",
    "            chunkGram=r\"\"\"Chunk:{<RB>?<VB>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser=nltk.regexpParser(chunkGram)\n",
    "            chunked=chunkParser.parse(tagged)\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c15c38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\tkrcet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in penn treeebank corpus: 3914\n",
      "\n",
      " Parse tree of first sentence: (S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n",
      "\n",
      " Subtree: (NP-SBJ\n",
      "  (NP (NNP Pierre) (NNP Vinken))\n",
      "  (, ,)\n",
      "  (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "  (, ,))\n",
      "Bot: Hi there! How can I help you today?\n",
      "You: hi\n",
      "Bot: Hey!\n",
      "You: exit\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "#week 13\n",
    "\n",
    "\n",
    "#exp 1\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "nltk.download('treebank')\n",
    "ptb=treebank.parsed_sents()\n",
    "print('Total sentences in penn treeebank corpus:',len(ptb))\n",
    "first_sent_tree=ptb[0]\n",
    "print('\\n Parse tree of first sentence:',first_sent_tree)\n",
    "subtree=first_sent_tree[0]\n",
    "print('\\n Subtree:',subtree)\n",
    "\n",
    "\n",
    "#exp 2\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "patterns = [\n",
    "    {\n",
    "        \"pattern\": r\"hi|hello|hey\",\n",
    "        \"responses\": [\"Hello!\", \"Hi there!\", \"Hey!\"]\n",
    "    },\n",
    "    {\n",
    "        \"pattern\": r\"how are you\",\n",
    "        \"responses\": [\"I'm good, thank you!\", \"Feeling great, thank you!\"]\n",
    "    },\n",
    "    {\n",
    "        \"pattern\": r\"bye|good bye\",\n",
    "        \"responses\": [\"Goodbye!\", \"See you later, bye!\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "def get_intent(text):\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern['pattern'], text.lower()):\n",
    "            return pattern['responses']\n",
    "    return [\"I'm sorry, I don't understand that.\"]\n",
    "\n",
    "print(\"Bot: Hi there! How can I help you today?\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print('Bot: Goodbye!')\n",
    "        break\n",
    "    responses = get_intent(user_input)\n",
    "    bot_response = random.choice(responses)\n",
    "    print(\"Bot:\", bot_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9d046bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  one/CD\n",
      "  (NP cat/NN)\n",
      "  chased/VBD\n",
      "  (NP a/DT mouse/NN)\n",
      "  in/IN\n",
      "  (NP the/DT garden/NN))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tkrcet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tkrcet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (VP (V chased) (NP (Det the) (N cat)))\n",
      "    (PP (P in) (NP (Det the) (N park)))))\n",
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V chased)\n",
      "    (NP (NP (Det the) (N cat)) (PP (P in) (NP (Det the) (N park))))))\n",
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (VP (V chased) (NP (Det the) (N cat)))\n",
      "    (PP (P in) (NP (Det the) (N park)))))\n",
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V chased)\n",
      "    (NP (NP (Det the) (N cat)) (PP (P in) (NP (Det the) (N park))))))\n",
      "predicates in sentence\n",
      "['is', 'sleeping']\n"
     ]
    }
   ],
   "source": [
    "#week 14\n",
    "\n",
    "\n",
    "#exp 1\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Ensure you have downloaded the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sentence to be processed\n",
    "sent = \"one cat chased a mouse in the garden\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "\n",
    "# POS tagging the tokens\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "# Define the grammar for chunking\n",
    "pattern = 'NP: {<DT>?<NN>*<NN>}'\n",
    "\n",
    "# Create a chunk parser with the specified pattern\n",
    "chunk_parser = nltk.RegexpParser(pattern)\n",
    "\n",
    "# Apply the chunk parser to the tagged tokens\n",
    "chunk_sentence = chunk_parser.parse(tagged)\n",
    "\n",
    "# Print the chunked sentence\n",
    "print(chunk_sentence)\n",
    "\n",
    "# Draw the chunk tree\n",
    "chunk_sentence.draw()\n",
    "\n",
    "\n",
    "#exp 2\n",
    "\n",
    "import nltk\n",
    "\n",
    "def top_down_parsing(grammar, sentence):\n",
    "    parser = nltk.ChartParser(grammar)\n",
    "    for tree in parser.parse(sentence.split()):\n",
    "        print(tree)\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det N | NP PP\n",
    "VP -> V NP | VP PP\n",
    "PP -> P NP\n",
    "Det -> 'the' | 'a'\n",
    "N -> 'dog' | 'cat' | 'man' | 'park'\n",
    "V -> 'chased' | 'saw'\n",
    "P -> 'in' | 'on'\n",
    "\"\"\")\n",
    "\n",
    "sentence = 'the dog chased the cat in the park'\n",
    "top_down_parsing(grammar, sentence)\n",
    "\n",
    "\n",
    "#exp 3\n",
    "\n",
    "import nltk\n",
    "\n",
    "def bottom_up_parsing(grammar, sentence):\n",
    "    parser = nltk.ChartParser(grammar)\n",
    "    for tree in parser.parse(sentence.split()):\n",
    "        print(tree)\n",
    "    \n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det N | NP PP\n",
    "VP -> V NP | VP PP\n",
    "PP -> P NP\n",
    "Det -> 'the' | 'a'\n",
    "N -> 'dog' | 'cat' | 'man' | 'park'\n",
    "V -> 'chased' | 'saw'\n",
    "P -> 'in' | 'on'\n",
    "\"\"\")\n",
    "\n",
    "sentence = 'the dog chased the cat in the park'\n",
    "bottom_up_parsing(grammar, sentence)\n",
    "\n",
    "\n",
    "#exp 4\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def get_predicates(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(tokens)\n",
    "    predicates = []\n",
    "    for word, tag in tagged_words:\n",
    "        if tag.startswith('VB'):\n",
    "            predicates.append(word)\n",
    "    return predicates\n",
    "\n",
    "sentence = 'The cat is sleeping on the mat'\n",
    "predicates = get_predicates(sentence)\n",
    "if predicates:\n",
    "    print('predicates in sentence')\n",
    "    print(predicates)\n",
    "else:\n",
    "    print('no predicates found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce154b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter num:5\n",
      "Number is prime\n",
      "Enter num:555\n",
      "Number is a palindrome\n",
      "Enter num: 5\n",
      "The factorial of 5 is 120\n"
     ]
    }
   ],
   "source": [
    "#week 15\n",
    "\n",
    "\n",
    "#exp 1\n",
    "\n",
    "def is_prime(n):\n",
    "    if n <= 1:\n",
    "        return False\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "num=int(input('Enter num:'))\n",
    "if is_prime(num):\n",
    "    print(\"Number is prime\")\n",
    "else:\n",
    "    print(\"number is not prime\")\n",
    "    \n",
    "#exp 2\n",
    "\n",
    "def is_palindrome(n):\n",
    "    return str(n)==str(n)[::-1]\n",
    "num=int(input(\"Enter num:\"))\n",
    "if is_palindrome(num):\n",
    "    print(\"Number is a palindrome\")\n",
    "else:\n",
    "    print(\"Number is not a palindrome\")\n",
    "    \n",
    "#exp 3\n",
    "\n",
    "def main():\n",
    "    num = get_input()\n",
    "    res = fact(num)\n",
    "    disp(num, res)\n",
    "\n",
    "def get_input():\n",
    "    return int(input(\"Enter num: \"))\n",
    "\n",
    "def fact(n):\n",
    "    if n == 0 or n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * fact(n - 1)\n",
    "\n",
    "def disp(num, res):\n",
    "    print(f\"The factorial of {num} is {res}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f050d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of ('class', 'based'): 0.1111111111111111\n",
      "Probability of ('sample', 'example'): 0.1111111111111111\n",
      "Variable values extracted from text:\n",
      "animals: fox\n",
      "action: jumps\n",
      "adjective: quick\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\vishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\names.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.281\n",
      "             2          -0.43290        0.725\n",
      "             3          -0.35111        0.903\n",
      "             4          -0.29492        0.995\n",
      "         Final          -0.25363        1.000\n",
      "Accuracy: 100.0%\n",
      "Predicted gender for John: female\n"
     ]
    }
   ],
   "source": [
    "#week 16\n",
    "\n",
    "\n",
    "#exp 1\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "class NgramModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.ngrams = defaultdict(int)\n",
    "        self.total_count = 0\n",
    "\n",
    "    def train(self, text):\n",
    "        words = re.findall(r'\\w+', text.lower())\n",
    "        for i in range(len(words) - self.n + 1):\n",
    "            ngram = tuple(words[i:i + self.n])\n",
    "            self.ngrams[ngram] += 1\n",
    "            self.total_count += 1\n",
    "\n",
    "    def prob(self, ngram):\n",
    "        return self.ngrams[ngram] / self.total_count if self.total_count > 0 else 0\n",
    "\n",
    "text = \"this is a sample example for a class based ngram\"\n",
    "n = 2\n",
    "model = NgramModel(n)\n",
    "model.train(text)\n",
    "\n",
    "# Define n-grams for probability calculation\n",
    "class_based = ('class', 'based')\n",
    "simple_example = ('sample', 'example')\n",
    "\n",
    "print(\"Probability of ('class', 'based'):\", model.prob(class_based))\n",
    "print(\"Probability of ('sample', 'example'):\", model.prob(simple_example))\n",
    "\n",
    "\n",
    "#exp 2\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "variables = {\n",
    "    \"animals\": r\"(fox|dog)\",\n",
    "    \"action\": r\"(jumps|runs)\",\n",
    "    \"adjective\": r\"(quick|lazy|brown)\"\n",
    "}\n",
    "\n",
    "variable_values = {}\n",
    "\n",
    "for var, pattern in variables.items():\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        variable_values[var] = match.group()\n",
    "\n",
    "print(\"Variable values extracted from text:\")\n",
    "for var, value in variable_values.items():\n",
    "    print(f\"{var}: {value}\")\n",
    "\n",
    "#exp 3\n",
    "\n",
    "import nltk\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.corpus import names as nltk_names\n",
    "nltk.download('names')\n",
    "\n",
    "names = [(name, 'male' if name.endswith('a') else 'female') for name in nltk_names.words()]\n",
    "\n",
    "def extract_features(name):\n",
    "    features = {}\n",
    "    features['last_letter'] = name[-1].lower()\n",
    "    features['length'] = len(name)\n",
    "    return features\n",
    "\n",
    "labeled_names = [(extract_features(name), gender) for name, gender in names]\n",
    "\n",
    "cutoff = int(0.8 * len(labeled_names))\n",
    "train_data, test_data = labeled_names[:cutoff], labeled_names[cutoff:]\n",
    "classifier = MaxentClassifier.train(train_data, max_iter=5)\n",
    "accuracy = nltk.classify.accuracy(classifier, test_data)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "\n",
    "new_name = \"John\"\n",
    "new_features = extract_features(new_name)\n",
    "print(f\"Predicted gender for {new_name}: {classifier.classify(new_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4435b027-0edd-472a-810c-8c45b695cfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
