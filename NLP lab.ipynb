{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4d6eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the first sentence.this is the second sentence.is this third\n",
      "this is the first sentence. this is the second sentence. is this third?\n",
      "['this', 'is', 'the', 'first', 'sentence', '.', 'this', 'is', 'the', 'second', 'sentence', '.', 'is', 'this', 'third', '?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#week1\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text1=\"this is the first sentence.this is the second sentence.is this third\"\n",
    "print(text1)\n",
    "sent_tokenize(text1)\n",
    "len(sent_tokenize(text1))\n",
    "text1=\"this is the first sentence. this is the second sentence. is this third?\"\n",
    "print(text1)\n",
    "sent_tokenize(text1)\n",
    "len(sent_tokenize(text1))\n",
    "word_tokenize(text1)\n",
    "len(word_tokenize(text1))\n",
    "word_result=word_tokenize(text1)\n",
    "print(word_result)\n",
    "word_result[1:5]\n",
    "word_result[:5]\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "pst=PunktSentenceTokenizer()\n",
    "pst.tokenize(text1)\n",
    "len(pst.tokenize(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5070baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Forms for : 'talk':talks, talked, talking\n",
      "Word Forms for : 'run':runs, runer, runing\n",
      "Word Forms for : 'jump':jumps, jumped, jumping\n",
      "Word Forms for : 'play':plays, player, playing\n"
     ]
    }
   ],
   "source": [
    "#week2\n",
    "\n",
    "word_suffixes={\n",
    "    \"talk\": [\"s\",\"ed\",\"ing\"],\n",
    "    \"run\" : [\"s\",\"er\",\"ing\"],\n",
    "    \"jump\" : [\"s\",\"ed\",\"ing\"],\n",
    "    \"play\" : [\"s\",\"er\",\"ing\"],\n",
    "}\n",
    "def generate_word_froms(root,suffixes):\n",
    "    word_forms=[root + suffix for suffix in suffixes]\n",
    "    return word_forms\n",
    "for root, suffixes in word_suffixes.items():\n",
    "    generated_forms= generate_word_froms(root,suffixes)\n",
    "    print(f\"Word Forms for : '{root}':{', '.join(generated_forms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a828c520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumps -> jump\n",
      "jumping -> jump\n",
      "played -> play\n",
      "playing -> play\n"
     ]
    }
   ],
   "source": [
    "#week3\n",
    "\n",
    "add_delete_table={\n",
    "    \"s\": \"\",\n",
    "    \"es\": \"\",\n",
    "    \"ed\": \"\",\n",
    "    \"ing\" : \"\",\n",
    "}\n",
    "def stem_word(word):\n",
    "    for suffix,replacement in add_delete_table.items():\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)] + replacement\n",
    "    return word\n",
    "\n",
    "words=[\"jumps\",\"jumping\",\"played\",\"playing\"]\n",
    "\n",
    "stemmed_words = [stem_word(word) for word in words]\n",
    "\n",
    "for word, stemmed in zip(words,stemmed_words):\n",
    "    print(f\"{word} -> {stemmed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da04f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: ['Indian', 'Institute', 'Technology', 'IIT']\n",
      "\n",
      "After Extracting the parts\n",
      "\n",
      " (S\n",
      "  (NP This/DT tree/NN)\n",
      "  (VP (V is/VBZ))\n",
      "  (VP\n",
      "    (V illustrating/VBG)\n",
      "    (NP the/DT constituency/NN)\n",
      "    (NP relation/NN))) \n",
      "\n",
      "Subject: Mary\n",
      "Verb: likes\n",
      "Object: Mary\n"
     ]
    }
   ],
   "source": [
    "#week4\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "# Sample text\n",
    "text = \"I visited the Indian Institute of Technology (IIT) last summer.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Part-of-speech tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "named_entities = nltk.ne_chunk(tagged_tokens)\n",
    "\n",
    "# Extracting named entities\n",
    "entities = []\n",
    "for entity in named_entities:\n",
    "    if isinstance(entity, nltk.Tree):\n",
    "        entities.append(\" \".join([word for word, tag in entity.leaves()]))\n",
    "print(\"Named Entities:\", entities)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "\n",
    "# String to parse\n",
    "to_parse = \"This tree is illustrating the constituency relation\"\n",
    "\n",
    "# Find all parts of speech in above sentence\n",
    "tagged_parts = pos_tag(word_tokenize(to_parse))\n",
    "\n",
    "# Defining grammar on basis of which we 've to extract\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}\n",
    "    P: {<IN>}\n",
    "    V: {<V.*>}\n",
    "    PP: {<p> <NP>}\n",
    "    VP: {<V> <NP|PP>*}\"\"\"\n",
    "\n",
    "# Extracting all parts of speech\n",
    "parser = RegexpParser(grammar)\n",
    "\n",
    "# Print all parts of speech in above sentence\n",
    "output = parser.parse(tagged_parts)\n",
    "print(\"\\nAfter Extracting the parts\\n\\n\", output,\"\\n\")\n",
    "\n",
    "\n",
    "#semantic parsing\n",
    "def extract_semantic_info(sentence):\n",
    "    words = sentence.split()\n",
    "    subject = \"\"\n",
    "    verb = \"\"\n",
    "    object_ = \"\"\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i].lower() in [\"he\", \"she\", \"it\", \"they\", \"john\", \"mary\"]:\n",
    "            subject = words[i]\n",
    "        elif words[i].lower() in [\"likes\", \"loves\", \"hates\", \"adores\"]:\n",
    "            verb = words[i]\n",
    "        if i+1 < len(words):\n",
    "            object_=words[i+1]\n",
    "\n",
    "    return subject, verb, object_\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"John likes Mary\"\n",
    "\n",
    "# Extract semantic information\n",
    "subject, verb, object_ = extract_semantic_info(sentence)\n",
    "\n",
    "# Print semantic information\n",
    "print(\"Subject:\", subject)\n",
    "print(\"Verb:\", verb)\n",
    "print(\"Object:\", object_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92b6b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['The', 'dogs', 'are', 'barking', 'loudly', 'outside', '.']\n",
      "Lemmatized tokens: ['The', 'dog', 'are', 'barking', 'loudly', 'outside', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Tutorials', 'Point']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#week5\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "# Sample text\n",
    "text = \"The dogs are barking loudly outside.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Initialize WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize tokens\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Lemmatized tokens:\", lemmatized_tokens)\n",
    "\n",
    "\n",
    "import os, os.path\n",
    "path = os.path.expanduser('~/natural_language_toolkit_data')\n",
    "if not os.path.exists(path):\n",
    "   os.mkdir(path)\n",
    "os.path.exists(path)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "exptagger = DefaultTagger('NN')\n",
    "exptagger.tag(['Tutorials','Point'])\n",
    "\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "exptagger = DefaultTagger('NN')\n",
    "exptagger.tag_sents([['Hi', ','], ['How', 'are', 'you', '?']])\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import untag\n",
    "untag([('Tutorials', 'NN'), ('Point', 'NN')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7c7d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize', 'this', 'sentence', 'using', 'NLTK', '.']\n",
      "[('This', 'DT'), ('is', 'VBZ'), ('simple', 'JJ'), ('POS', 'NNP'), ('tagging', 'VBG'), ('example', 'NN'), ('.', '.')]\n",
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP City/NNP)\n",
      "  ./.)\n",
      "Neutral Sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#week6\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence= \"Tokenize this sentence using NLTK.\"\n",
    "tokens=word_tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence=\"This is simple POS tagging example.\"\n",
    "tokens=word_tokenize(sentence)\n",
    "tagged=nltk.pos_tag(tokens)\n",
    "print(tagged)\n",
    "\n",
    "\n",
    "#NAMED ENTITY RECOGNITION\n",
    "import nltk\n",
    "sentence=\"John works at Google in New York City.\"\n",
    "tokens=nltk.word_tokenize(sentence)\n",
    "tagged=nltk.pos_tag(tokens)\n",
    "entities=nltk.ne_chunk(tagged)\n",
    "print(entities)\n",
    "\n",
    "\n",
    "#SENTIMENTAL ANAKLYSIS\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sentence=\"This is a good movie.\"\n",
    "analyzer=SentimentIntensityAnalyzer()\n",
    "sentiment_scores=analyzer.polarity_scores(sentence)\n",
    "\n",
    "if sentiment_scores['compound']>=0.5:\n",
    "    print(\"Positive Sentiment\")\n",
    "elif sentiment_scores['compound']<=-0.5:\n",
    "    print(\"Negative Sentiment\")\n",
    "else:\n",
    "    print(\"Neutral Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5065c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Machine', 'learning'), ('learning', 'is'), ('is', 'an'), ('an', 'important'), ('important', 'part'), ('part', 'of'), ('of', 'AIand'), ('AIand', 'AI'), ('AI', 'is'), ('is', 'going'), ('going', 'to'), ('to', 'become'), ('become', 'inmportant'), ('inmportant', 'for'), ('for', 'daily'), ('daily', 'functionong')]\n",
      "['BBDO South', 'Georgia-Pacific']\n",
      "['This', 'is', 'a', 'dog', 'This', 'is', 'a', 'cat', 'I', 'love', 'my', 'cat', 'This', 'is', 'my', 'name']\n",
      "\n",
      " All the possible Bigrams are \n",
      "[('This', 'is'), ('is', 'a'), ('a', 'dog'), ('This', 'is'), ('is', 'a'), ('a', 'cat'), ('I', 'love'), ('love', 'my'), ('my', 'cat'), ('This', 'is'), ('is', 'my'), ('my', 'name')]\n",
      "\n",
      " Bigrams along with their frequency \n",
      "{('This', 'is'): 3, ('is', 'a'): 2, ('a', 'dog'): 1, ('a', 'cat'): 1, ('I', 'love'): 1, ('love', 'my'): 1, ('my', 'cat'): 1, ('is', 'my'): 1, ('my', 'name'): 1}\n",
      "\n",
      " Unigrams along with their frequency \n",
      "{'This': 3, 'is': 3, 'a': 2, 'dog': 1, 'cat': 2, 'I': 1, 'love': 1, 'my': 2}\n",
      "\n",
      " Bigrams along with their probability\n",
      "{('This', 'is'): 1.0, ('is', 'a'): 0.6666666666666666, ('a', 'dog'): 0.5, ('a', 'cat'): 0.5, ('I', 'love'): 1.0, ('love', 'my'): 1.0, ('my', 'cat'): 0.5, ('is', 'my'): 0.3333333333333333, ('my', 'name'): 0.5}\n",
      "\n",
      " The bigrams in the given sentence are\n",
      "[('This', 'is'), ('is', 'my'), ('my', 'cat')]\n",
      "\n",
      "Probability of the sentence \"This is my cat\" = 0.16666666666666666\n",
      "0.05555555555555555\n",
      "('<s>', 'John') 1 3\n",
      "('John', 'read') 1 1\n",
      "('read', 'a') 2 3\n",
      "('a', 'book') 1 2\n",
      "('book', '</s>') 1 2\n"
     ]
    }
   ],
   "source": [
    "#week7\n",
    "\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "s=\"Machine learning is an important part of AI\"\"and AI is going to become inmportant for daily functionong\" \n",
    "tokens=[token for token in s.split(\" \")]\n",
    "output=list(ngrams(tokens,2))\n",
    "print(output)\n",
    "\n",
    "locs = [('Omnicom', 'IN', 'New York'), ('DDB Needham', 'IN', 'New York'), ('Kaplan Thaler Group', 'IN', 'New York'), ('BBDO South', 'IN', 'Atlanta'), ('Georgia-Pacific', 'IN', 'Atlanta')]\n",
    "query = [e1 for (e1, rel, e2) in locs if e2 == 'Atlanta']\n",
    "print(query)\n",
    "\n",
    "\n",
    "def readData():\n",
    "    data = ['This is a dog', 'This is a cat', 'I love my cat', 'This is my name']\n",
    "    dat = []\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i].split():\n",
    "            dat.append(word)\n",
    "    print(dat)\n",
    "    return dat\n",
    "\n",
    "def createBigram(data):\n",
    "    listOfBigrams = []\n",
    "    bigramCounts = {}\n",
    "    unigramCounts = {}\n",
    "    for i in range(len(data)-1):\n",
    "      if i <len(data)-1 and data[i+1].islower():\n",
    "          listOfBigrams.append((data[i],data[i+1]))\n",
    "          if(data[i],data[i+1]) in bigramCounts:\n",
    "            bigramCounts[(data[i],data[i+1])] += 1\n",
    "          else:\n",
    "            bigramCounts[(data[i],data[i+1])] = 1\n",
    "      if data[i] in unigramCounts:\n",
    "            unigramCounts[data[i]] += 1\n",
    "      else:\n",
    "            unigramCounts[data[i]] = 1\n",
    "    return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1= bigram[0]\n",
    "        word2= bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram)) /(unigramCounts.get(word1))\n",
    "    return listOfProb\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data=readData()\n",
    "    listOfBigrams, unigramCounts, bigramCounts=createBigram(data)\n",
    "    \n",
    "    print(\"\\n All the possible Bigrams are \")\n",
    "    print(listOfBigrams)\n",
    "    print(\"\\n Bigrams along with their frequency \")\n",
    "    print(bigramCounts)\n",
    "    print(\"\\n Unigrams along with their frequency \")\n",
    "    print(unigramCounts)\n",
    "    bigramProb=calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "    print(\"\\n Bigrams along with their probability\")\n",
    "    print(bigramProb)\n",
    "    inputList=\"This is my cat\"\n",
    "    splt=inputList.split()\n",
    "    outputProb1= 1\n",
    "    bilist= []\n",
    "    bigrm =[]\n",
    "    for i in range (len(splt) - 1):\n",
    "        if i < len(splt) - 1:\n",
    "            bilist.append((splt[i], splt[i+1]))\n",
    "    print(\"\\n The bigrams in the given sentence are\")\n",
    "    print(bilist)\n",
    "\n",
    "    outputProb1 = 1  # Start with a probability of 1\n",
    "    for bigram in bilist:\n",
    "        if bigram in bigramProb:\n",
    "            outputProb1 *= bigramProb[bigram]  # Multiply by the probability of the bigram if it exists\n",
    "        else:\n",
    "            outputProb1 *= 0  # Multiply by 0 if the bigram does not exist in the model\n",
    "\n",
    "    print('\\n' + 'Probability of the sentence \"This is my cat\" = ' + str(outputProb1))\n",
    "\n",
    "    \n",
    "\n",
    "from collections import Counter\n",
    "from functools import reduce, partial\n",
    "from operator import mul\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "def prob_product(prob_list):\n",
    "  return reduce(mul, prob_list)\n",
    "\n",
    "text = [['<s>', 'John', 'read', 'Moby', 'Dick', '</s>'],\n",
    "        ['<s>', 'Mary', 'read', 'a', 'different', 'book', '</s>'],\n",
    "        ['<s>', 'She', 'read', 'a', 'book', 'by', 'Cher', '</s>']]\n",
    "\n",
    "bigram_counts=sum([Counter(ngrams(t, 2)) for t in text], Counter())\n",
    "unigram_counts=sum([Counter(ngrams(t, 1)) for t in text], Counter())\n",
    "\n",
    "count_S_John= bigram_counts[('<s>', 'John')]\n",
    "count_S=unigram_counts[('<s>',)]\n",
    "\n",
    "sentence = '<s> John read a book </s>'.split()\n",
    "\n",
    "prob_S_John_read_a_book=prob_product([bigram_counts[bg]/unigram_counts[bg[:-1]] \n",
    "                                      for bg in ngrams(sentence,2)])\n",
    "print(prob_S_John_read_a_book)\n",
    "\n",
    "for bg in ngrams(sentence,2):\n",
    "  print(bg,bigram_counts[bg], unigram_counts[bg[:-1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
